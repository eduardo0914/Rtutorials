---
title: People Analytics - Employee Engagement
author: Eduardo Villarreal
date: '2022-05-14'
slug: people-analytics-employee-engagement
categories:
  - Machine Learning
  - People Analytics
tags:
  - Classification
  - Machine Learning
  - Employee Engagement
---



<div id="what-is-employee-engagement" class="section level1">
<h1>What is Employee Engagement</h1>
<p><a href="https://en.wikipedia.org/wiki/Employee_engagement">Wikipwdia-Employee Engagement</a></p>
<p>Employee engagement is a fundamental concept in the effort to understand and describe, both qualitatively and quantitatively, the nature of the relationship between an organization and its employees. An “engaged employee” is defined as one who is fully absorbed by and enthusiastic about their work and so takes positive action to further the organization’s reputation and interests. An engaged employee has a positive attitude towards the organization and its values. In contrast, a disengaged employee may range from someone doing the bare minimum at work (aka ‘coasting’), up to an employee who is actively damaging the company’s work output and reputation.</p>
<p>An organization with “high” employee engagement might therefore be expected to outperform those with “low” employee engagement.</p>
<p>Employee engagement first appeared as a concept in management theory in the 1990s, becoming widespread in management practice in the 2000s, but it remains contested. It stands in an unspecified relationship to earlier constructs such as morale and job satisfaction. Despite academic critiques, employee engagement practices are well established in the management of human resources and of internal communications.</p>
<p>William Kahn provided the first formal definition of personnel engagement as “the harnessing of organisation members’ selves to their work roles; in engagement, people employ and express themselves physically, cognitively, and emotionally during role performances.</p>
<p>In 1993, Schmidt et al. proposed a bridge between the pre-existing concept of ‘<a href="https://en.wikipedia.org/wiki/Job_satisfaction" title="Job satisfaction">job satisfaction</a>’ and employee engagement with the definition: “an employee’s involvement with, commitment to, and satisfaction with work. Employee engagement is a part of employee retention.” This definition integrates the classic constructs of job satisfaction (Smith et al., 1969), and organizational commitment (Meyer &amp; Allen, 1991).</p>
<p>Defining employee engagement remains problematic. In their review of the literature in 2011, Wollard and Shuck identify four main sub-concepts within the term:</p>
<ol style="list-style-type: decimal">
<li><p>“Needs satisfying” approach, in which engagement is the expression of one’s preferred self in task behaviours.</p></li>
<li><p>“Burnout antithesis” approach, in which energy, involvement, efficacy are presented as the opposites of established “burnout” constructs: exhaustion, cynicism and lack of accomplishment.</p></li>
<li><p>Satisfaction-engagement approach, in which engagement is a more technical version of job satisfaction, evidenced by <a href="https://en.wikipedia.org/wiki/Gallup_(company)" title="Gallup (company)">The Gallup Company’s</a> own Q12 engagement survey which gives an r=.91 correlation with one (job satisfaction) measure.</p></li>
<li><p>The multidimensional approach, in which a clear distinction is maintained between job and organisational engagement, usually with the primary focus on antecedents and consequents to role performance rather than organisational identification.</p></li>
</ol>
<p>Definitions of engagement vary in the weight they give to the individual vs the organisation in creating engagement. Recent practice has situated the drivers of engagement across this spectrum, from within the psyche of the individual employee (for example, promising recruitment services that will filter out ‘disengaged’ job applicants) to focusing mainly on the actions and investments the organisation makes to support engagement.</p>
<p>These definitional issues are potentially severe for practitioners. With different (and often proprietary) definitions of the object being measured, statistics from different sources are not readily comparable. Engagement work remains open to the challenge that its basic assumptions are, as Tom Keenoy describes them, ‘<a href="https://en.wikipedia.org/wiki/Norm_(philosophy)" title="Norm (philosophy)">normative</a>’ and ‘aspirational’, rather than <a href="https://en.wikipedia.org/wiki/Analytics" title="Analytics">analytic</a> or operational - and so risk being seen by other organizational participants as “motherhood and apple pie” rhetoric.</p>
</div>
<div id="description-of-the-data" class="section level1">
<h1>Description of the Data</h1>
<p>The data set describes ~2500 Managers rated by their employees. Usually this type of survey data, has a big amount of questions divided in different dimentions such as Rewards, Recognition, Empowerment, Diveristy &amp; Inclussion, etc.</p>
<p>Companies rely on annual engagement surveys to track and monitor the motivation of employees. Engagement is important because it is shown that it correlates to productivity and company performance. High levels of engagmenet are also related to low turn over rates and a strong Employee Vale Proposition.</p>
<pre class="r"><code>knitr::opts_chunk$set(echo = TRUE)

#Cargar librerias
require(tidyverse)
require(data.table)
require(tidymodels)
require(visreg)
require(arules)
require(patchwork)

theme_set(theme_bw())
tidymodels_prefer()</code></pre>
<pre class="r"><code>engagement = fread(&#39;Engagement.csv&#39;)
glimpse(engagement)</code></pre>
<pre><code>## Rows: 197,352
## Columns: 10
## $ id       &lt;int&gt; 94100812, 94253692, 97920505, 32067026, 32174057, 32167306, 8…
## $ Year     &lt;int&gt; 2021, 2021, 2021, 2021, 2021, 2021, 2021, 2021, 2021, 2021, 2…
## $ clasif   &lt;chr&gt; &quot;Annual&quot;, &quot;Annual&quot;, &quot;Annual&quot;, &quot;Annual&quot;, &quot;Annual&quot;, &quot;Annual&quot;, &quot;…
## $ engTotal &lt;dbl&gt; 60.29310, 86.13793, 44.86207, 87.96552, 63.62069, 95.68966, 5…
## $ equipo   &lt;int&gt; 6, 10, 6, 18, 7, 19, 21, 22, 5, 10, 13, 42, 6, 27, 7, 6, 8, 7…
## $ IdCode   &lt;chr&gt; &quot;G1&quot;, &quot;G1&quot;, &quot;G1&quot;, &quot;G1&quot;, &quot;G1&quot;, &quot;G1&quot;, &quot;G1&quot;, &quot;G1&quot;, &quot;G1&quot;, &quot;G1&quot;, &quot;…
## $ Value    &lt;int&gt; 83, 98, 38, 94, 64, 99, 76, 97, 75, 100, 96, 88, 96, 92, 100,…
## $ Gender   &lt;chr&gt; &quot;Male&quot;, &quot;Female&quot;, &quot;Female&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;…
## $ Age      &lt;int&gt; 65, 27, 42, 42, 33, 26, 41, 25, 34, 37, 59, 37, 39, 48, 52, 3…
## $ Grade    &lt;chr&gt; &quot;VIII&quot;, &quot;VIII&quot;, &quot;VIII&quot;, &quot;VII&quot;, &quot;VI&quot;, &quot;VII&quot;, &quot;VII&quot;, &quot;VIII&quot;, &quot;V…</code></pre>
<p>The column <code>id</code> relates each employee with an unique identifier, <code>IdCode</code> stores each question on the survey s well as the result for each dimention. In this case <code>G1</code> relates to the engagement index.</p>
<p>Prior building an algorithm, we need to reshape the data so we can manipulate it in an efficient way.</p>
<div id="reshaping-the-data" class="section level2">
<h2>Reshaping the Data</h2>
<p>I´m going to split the data in 2 big dataframes:</p>
<ol style="list-style-type: decimal">
<li><p>Containing the set of questions</p></li>
<li><p>A second one with the demographic data available on the data set</p></li>
</ol>
<p>Then, I´m hing to merge both data sets to have the final data frame.</p>
<pre class="r"><code>engagement_surv = engagement %&gt;%
  select(id, IdCode, Value) %&gt;%
  spread(IdCode, Value, fill = NA)

engagement_emp = engagement %&gt;%
  select(id,equipo, Gender, Age, Grade) %&gt;%
  unique() #Remove duplicated rows

message(&#39;Merging Data setd===============================================&#39;)</code></pre>
<pre><code>## Merging Data setd===============================================</code></pre>
<pre class="r"><code>engagement_df = left_join(engagement_surv, engagement_emp)</code></pre>
<pre><code>## Joining, by = &quot;id&quot;</code></pre>
<pre class="r"><code>head(engagement_df)</code></pre>
<pre><code>##          id G1 G10 G11 G12 G13 G14  G2  G3  G4  G5 G6  G7 G8 G9  P1 P10 P11 P12
## 1: 10171514 96  96  92 100 100  83 100  87  89  67 92  83 87 93 100  83  83  67
## 2: 10714418 96  98  83 100  71  71  74  89 100  86 82  94 74 94 100 100  71  71
## 3: 10714429 75  75  65  92  50  58  73  85  88  81 73  85 62 90  83  75  75  83
## 4: 12025062 94  92  96 100  78  67  91  96  84  89 94  98 80 98 100 100  89  89
## 5: 29002499 95  90  87 100 100  80  96 100  92  87 95 100 80 96 100 100 100 100
## 6: 29005261 75  78  80 100 100  80  96 100  88 100 80  80 72 96 100 100 100 100
##    P13 P14 P15 P16 P17 P18 P19  P2 P20 P21 P22 P23 P24 P25 P26 P27 P28 P29  P3
## 1: 100 100 100 100  80  83  80  83  67  67  67  83 100 100  83 100  50  83 100
## 2: 100 100 100 100 100 100 100 100  86  86  86  57  86 100  86 100  71 100  86
## 3: 100  92  92  83  83  92  92  58  75  92  75  50  83  83  75 100  83  83  92
## 4: 100 100 100  78  78  78  89  89  89  89  89 100  89 100  89 100 100  89  89
## 5: 100 100 100 100  80 100  80  80  80 100  80  80 100 100 100 100 100 100 100
## 6: 100 100 100  80  80 100  80  60 100 100 100  80  60 100  80  80  80  60  80
##    P30 P31 P32 P33 P34 P35 P36 P37 P38 P39  P4 P40 P41 P42 P43 P44 P45 P46 P47
## 1: 100  83 100  83  67  83 100 100 100 100 100 100  67  83 100 100 100 100 100
## 2: 100 100  71 100  57  86  57 100 100  86 100  86 100 100  86 100 100 100 100
## 3:  92  67  58  67  58  67  58 100  92  83  67  92  83  92  50  83  75  67  92
## 4: 100 100  67  89  78  89  78 100 100 100 100  89 100  89  89  89 100  78 100
## 5: 100 100  80  80  60 100  80 100 100 100 100 100  80  80  80 100  80 100 100
## 6:  80 100  80  80  60  60  80 100 100  80  60 100 100  60  60 100  60 100 100
##    P48 P49  P5 P50 P51 P52 P53 P54 P55 P56 P57 P58  P6  P7  P8  P9 equipo
## 1: 100  83 100 100 100  67 100  83 100 100 100  83 100 100 100 100      6
## 2: 100 100  71  71  83  57 100  86 100 100  71  71  71  71  86  71      7
## 3:  75  67  75  75  67  42  58  67  83  92  50  58  58  75  75  83     12
## 4: 100  89 100 100  89 100 100  89 100 100  78  67  67 100  89 100      9
## 5:  80 100 100 100  60  60 100 100 100 100 100  80  80 100 100 100      5
## 6:  80  60 100 100  60  80  80  60 100 100 100  80  80 100 100 100      5
##    Gender Age                Grade
## 1:   Male  53 00 Senior Management
## 2:   Male  40 00 Senior Management
## 3:   Male  61 00 Senior Management
## 4:   Male  46 00 Senior Management
## 5:   Male  48 00 Senior Management
## 6:   Male  45 00 Senior Management</code></pre>
</div>
<div id="pre-process-the-data" class="section level2">
<h2>Pre-process the Data</h2>
<pre class="r"><code>engagement_df = engagement_df %&gt;%
  mutate(Age_d = arules::discretize(Age)) %&gt;%
  filter(Grade != &#39;EBM&#39;) %&gt;%
  na.omit()</code></pre>
</div>
<div id="exploratory-data-analysis" class="section level2">
<h2>Exploratory Data Analysis</h2>
<p>Let´s figure out about the Data! The Output Variable is the <strong>General Engagement Index</strong> sotred in column <code>G1</code></p>
<pre class="r"><code>#Create a histogram
p1 = engagement_df %&gt;%
  ggplot(aes(x = G1)) +
  geom_histogram(color = &#39;white&#39;, fill = &#39;orange&#39;) +
  labs(title = &#39;Engagement Distribution&#39;)

p2 = engagement_df %&gt;%
  ggplot(aes(x = Gender, y =G1, fill = Gender)) +
  geom_boxplot() +
  labs(title = &#39;Boxplot by Gender&#39;) +
  theme(legend.position = &#39;none&#39;)

p3 = engagement_df %&gt;%
  ggplot(aes(x = Age_d, y = G1, fill = Age_d)) +
  geom_boxplot() +
  labs(title = &#39;Boxplot by Age&#39;) +
  theme(legend.position = &#39;none&#39;)

p4 = engagement_df %&gt;%
  ggplot(aes(x = Grade, y = G1, fill = Grade)) +
  geom_boxplot() +
  theme(legend.position = &#39;none&#39;)

(p1 | p2) / (p3 | p4)</code></pre>
<pre><code>## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>Now, lets figure out the correlation with the 11 Dimentions of engagement:</p>
<pre class="r"><code>engagement_df %&gt;%
  select(id, starts_with(&#39;G&#39;), -Grade, - Gender) %&gt;%
  gather(Variable, Value, 3 : ncol(.)) %&gt;%
  ggplot(aes(x = Value, y = G1, color = Variable)) +
  geom_point(alpha = 0.2) +
  facet_wrap(~Variable, scale = &#39;free&#39;) +
  theme(legend.position = &#39;none&#39;) +
  geom_smooth(color = &#39;grey40&#39;, se = F, lty = 2)</code></pre>
<pre><code>## `geom_smooth()` using method = &#39;gam&#39; and formula &#39;y ~ s(x, bs = &quot;cs&quot;)&#39;</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<pre class="r"><code>message(&#39;Correlation plot ===============================================&#39;)</code></pre>
<pre><code>## Correlation plot ===============================================</code></pre>
<pre class="r"><code>library(corrplot)</code></pre>
<pre><code>## corrplot 0.92 loaded</code></pre>
<pre class="r"><code>#Create correlation Matrix
corr_mat = engagement_df %&gt;%
  select(is.numeric, -id) %&gt;%
  as.matrix() %&gt;%
  cor()</code></pre>
<pre><code>## Warning: Predicate functions must be wrapped in `where()`.
## 
##   # Bad
##   data %&gt;% select(is.numeric)
## 
##   # Good
##   data %&gt;% select(where(is.numeric))
## 
## ℹ Please update your code.
## This message is displayed once per session.</code></pre>
<pre class="r"><code>corrplot(corr_mat, method = &#39;color&#39;, order = &#39;hclust&#39;, tl.cex = 0.4)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-6-2.png" width="672" /></p>
<p>Before continue, you can check the <a href="https://eduardo0914rtutorials.netlify.app/post/2022/05/08/introduction-to-statistical-learning-regression-and-regularization/">Introduction to Statistical Learning Tutorial</a>.</p>
</div>
</div>
<div id="logistic-regression-and-classification" class="section level1">
<h1>Logistic Regression and Classification</h1>
<p>The Engagement Score is a number between <span class="math inline">\([0,100]\)</span> very similar to a <strong>probability score</strong>. How ever, we need to define what is good engagement. It is someone with a 51% engagement? maybe 80%?. Let´s tdefine a treshold on our Average Engagement Score:</p>
<pre class="r"><code>engagement_df %&gt;%
  summarise(mean = mean(G1))</code></pre>
<pre><code>##       mean
## 1 89.55309</code></pre>
<p>This trashold makes sence form the business perspective. If our goal is to sustain or improve our score, we wold like to invstigate people over the average to lear what they are doing that other aren´t.</p>
<pre class="r"><code>engagement_df = engagement_df %&gt;%
  mutate(ENG = ifelse(G1 &gt;= 89, 1, 0))

require(ggmosaic)</code></pre>
<pre><code>## Loading required package: ggmosaic</code></pre>
<pre class="r"><code>#Plot ENG
engagement_df %&gt;%
  ggplot() +
  geom_mosaic(aes(x = product(Grade, ENG), fill = Grade)) +
  theme(legend.position = &#39;none&#39;) +
  labs(title = &#39;Mosaic plot of ENG by Grade&#39;)</code></pre>
<pre><code>## Warning: `unite_()` was deprecated in tidyr 1.2.0.
## Please use `unite()` instead.
## This warning is displayed once every 8 hours.
## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated.</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<div id="generalized-linear-models" class="section level2">
<h2>Generalized Linear Models</h2>
<p>A model is just a simple abstraction of reality in that it provides an approximation of some relatively more complex phenomenon. Models may be broadly classified as deterministic or probabilistic.</p>
<p>There is certainly no more important class of probabilistic model than the probabilistic linear model:</p>
<p><span class="math display">\[y_i = \beta_0 + \sum_{i = 1} ^ p \beta_i x_i + \epsilon\]</span></p>
<p>is called a linear model because the mean response is a linear function of the unknown parameters. More even, the <strong>Derivative</strong> respect to the <strong>Parameters</strong> is linear.</p>
<p><span class="math display">\[\frac{\partial y_i}{\partial \beta_i} = \beta_i\]</span></p>
<p>A second-order polinomial is also linear:</p>
<p><span class="math display">\[y_i = \beta_0 + \sum_{i = 1} ^ p \beta_i x_i + \sum_{i = 1} ^ p \beta_{ii}x_i ^2\]</span></p>
<p><span class="math display">\[\frac{\partial y_i}{\partial \beta_i} = \beta_i + 2 \beta_{ii}x_i\]</span></p>
<p>or even a model with transcendental terms such as:</p>
<p><span class="math display">\[y_i = \beta_0 + \beta_1 \sin(x_1) + \beta_2 \cos{x_2}\]</span></p>
<p>When the model is <strong>Linear</strong> the method of <strong>least square</strong>s is a parameter estimation technique that dates from the early part of the nineteenth century. There is a really nice, elegant, and well-developed statistical theory for the linear model. If we assume that the errors <span class="math inline">\(\epsilon\)</span> in the linear model are normally and independently distributed with constant variance, then statistical tests on the model parameters, confidence intervals on the parameters, and confidence and prediction intervals for the mean response can easily be obtained.</p>
<p>However, there are situations where the phenomenon is well understood and can be described by a mathematical relationship. For example, consider <strong>Newton’s law of cooling</strong>, which states that the rate of change of temperature of an object is proportional to the difference between the object’s current temperature and the temperature of the surrounding environment:</p>
<p><span class="math display">\[\frac{\partial f}{\partial t} = -\beta (f - T_A)\]</span> Solving the diferential equation we have the actual temperatur at time <em>t</em>:</p>
<p><span class="math display">\[f(t, \beta) = T_A + (T_I - T_A) e ^ {\beta t}\]</span></p>
<p>This equation is an example of a <strong>nonlinear model</strong>, because the response is not a linear function of the unknown parameter.</p>
<p>Suppose that the response variable is adiscrete variable, such as a <strong>count</strong>. We often encounter counts of defects or other rare events, such as injuries, patients with particular diseases, and even the occurrence of natural phenomena including earthquakes and Atlantic hurricanes.</p>
<p>Another possibility is a <strong>binary</strong> response variable. Situations where the response variable is either success or failure (i.e., 0 or 1 ) are fairly common in nearly all areas of science and engineering.</p>
<p>There are also situations where the response variable is continuous, but the assumption of normality is not reasonable. Examples include the distribution of stresses in mechanical components and the failure times of systems or components. These types of responses are nonnegative and typically have a highly right-skewed behavior.</p>
<p>The <strong>generalized linear model or (GLM)</strong> allows us to fit regression models for univariate response data that follow a very general distribution called the <strong>exponential</strong> family. The exponential family includes the normal, binomial, Poisson, geometric, negative binomial, exponential, gamma, and inverse normal distributions:</p>
<p><span class="math display">\[g(\mu_i) = g[E(y_i)] = X^T \beta\]</span></p>
<p>Every generalized linear model has three components: a <strong>response variable distribution</strong> (sometimes called the error structure), a <strong>linear predictor</strong> that involves the regressor variables or covariates, and a <strong>link function</strong> <span class="math inline">\(g\)</span> that connects the linear predictor to the natural mean of the response variable.</p>
<p>For example, consider the linear regression model in Equation. The response distribution is normal, the linear predictor is:</p>
<p><span class="math display">\[X^T\beta = \beta_0 + \sum_{i = 1} ^p \beta_i x_1\]</span></p>
<p>and the link function is an identity link, <span class="math inline">\(g(a) = a\)</span>, or</p>
<p><span class="math display">\[E[y] = u = \beta_0 + \sum_{i = 1} ^p \beta_i x_1\]</span></p>
<p>Thus the standard linear regression model is a <strong>GLM</strong>. Depending on the choice of the link function <span class="math inline">\(g\)</span>, a GLM can include a nonlinear model. For example, if we use a <strong>log link</strong>, <span class="math inline">\(g(a) = ln(a)\)</span>, then</p>
<p><span class="math display">\[E[y] = u = exp\left(\beta_0 + \sum_{i = 1} ^p \beta_i x_1 \right)\]</span></p>
<p>For the case of a <strong>binomial</strong> distribution, a fairly standard choice of link function is the <strong>logit link</strong></p>
<p><span class="math display">\[E[y] = \frac{exp(\beta_0 + \beta_1 x_1)}{1 + exp(\beta_0 + \beta_1 x_1)}\]</span></p>
</div>
<div id="binary-response-model-logistic-regression" class="section level2">
<h2>Binary Response Model (Logistic Regression)</h2>
<p>Consider the case in witch the response variable can take only 2 posibles outcomes (0, 1). It is reasonable to assume that <span class="math inline">\(y_i\)</span> is a <em>Bernulli</em> random variable:</p>
<p><span class="math display">\[E[y_i] = \pi_i = \pi(x_i)\]</span></p>
<p>and</p>
<p><span class="math display">\[Var[y_i] = \pi_i(1 - \pi_i)\]</span></p>
<p>Supposed that the model has the form:</p>
<p><span class="math display">\[y_i = x \beta + \epsilon\]</span></p>
<p>We assume that the response variable <span class="math inline">\(y\)</span>; is a Bernoulli random variable with probability distribution as follows:</p>
<p><span class="math display">\[P(y_i = 1) = \pi_i \rightarrow y_i = 1\]</span></p>
<p><span class="math display">\[P(y_i = 0) = 1 - \pi_i \rightarrow y_i = 0\]</span></p>
<p>Thus <span class="math inline">\(E[y_i] = x \beta = \pi_i\)</span> were $0 \pi_i $. This restriction on the <strong>response</strong> implies that the random terms <span class="math inline">\(\epsilon_i\)</span> arean´t normaly distributed.</p>
<p>Generally, when the response is <strong>Binary</strong> the equation is on the form:</p>
<p><span class="math display">\[E[y] = \frac{e^{x \beta}}{1 - e^{x \beta}}\]</span></p>
<p>However, the response can be linnearized as well. Let:</p>
<p><span class="math display">\[\eta = x \beta\]</span></p>
<p>be the linear predictor <span class="math inline">\(\eta\)</span> define as:</p>
<p><span class="math display">\[\eta = ln \frac{\pi}{1 - \pi}\]</span></p>
<p>This is called the <strong>logit</strong> transformation of the probabiity <span class="math inline">\(\pi\)</span>. This follows a <em>Bernulli</em> process:</p>
<p><span class="math display">\[E[y_i] = \pi_i^{y_i} (1 - \pi_i)^{1 - y_i}\]</span></p>
<p>Then, is easy to applied <strong>Maximunlikelihood</strong> theroy to stimate the model parameters:</p>
<p><span class="math display">\[\mathscr{L}(\beta, y_i) = \prod_{i=1}^nf(y_i) = \prod_{i=1}^n\pi_i^{y_i} (1 - \pi_i)^{1 - y_i}\]</span></p>
<p>witch is more convenient to work in Logarithms:</p>
<p><span class="math display">\[ln \mathscr{L}(\beta, y_i) = \sum_{i=1}^n y_i \big[ ln \frac{\pi_i}{1 - \pi_i} \big ] +
\sum_{i=1}^n ln [1-\pi_i]\]</span></p>
<p>Non linnear optimizatoin routines are neeed in order to find the optimal parameters.</p>
<p>Now we are ready to estimate our model:</p>
<pre class="r"><code>#Split data for training and test
eng_split = initial_split(engagement_df)
eng_train = training(eng_split)
eng_test = testing(eng_split)

message(&#39;using glm to with family = binomial ===========================&#39;)</code></pre>
<pre><code>## using glm to with family = binomial ===========================</code></pre>
<pre class="r"><code>eng_glm = glm(ENG ~ G2 + G3 + G4 + G5 + G6 + G7 + G8 +
                G9 + G10 + G11 + Age_d + Gender ,
              data = eng_train, family = &#39;binomial&#39;)


tidy(eng_glm)</code></pre>
<pre><code>## # A tibble: 14 × 5
##    term          estimate std.error statistic  p.value
##    &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
##  1 (Intercept)  -27.3       1.36     -20.1    4.84e-90
##  2 G2             0.00499   0.0102     0.487  6.26e- 1
##  3 G3             0.00555   0.0169     0.329  7.42e- 1
##  4 G4             0.186     0.0147    12.7    1.11e-36
##  5 G5            -0.0315    0.0104    -3.02   2.55e- 3
##  6 G6            -0.00945   0.0134    -0.705  4.81e- 1
##  7 G7             0.0234    0.0132     1.77   7.64e- 2
##  8 G8             0.0141    0.00954    1.48   1.39e- 1
##  9 G9            -0.0424    0.0128    -3.31   9.20e- 4
## 10 G10            0.0895    0.0178     5.04   4.73e- 7
## 11 G11            0.0785    0.0130     6.04   1.52e- 9
## 12 Age_d[34,42)  -0.00666   0.186     -0.0358 9.71e- 1
## 13 Age_d[42,71]  -0.0246    0.185     -0.133  8.94e- 1
## 14 GenderMale    -0.0748    0.206     -0.363  7.16e- 1</code></pre>
<p>A more clear interpretation is on terms of the <strong>odds ratio</strong> defined as:</p>
<p><span class="math display">\[\text{odds}_i = e ^ {\beta_i}\]</span></p>
<pre class="r"><code>tidy(eng_glm, exponentiate = T)</code></pre>
<pre><code>## # A tibble: 14 × 5
##    term         estimate std.error statistic  p.value
##    &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
##  1 (Intercept)  1.34e-12   1.36     -20.1    4.84e-90
##  2 G2           1.01e+ 0   0.0102     0.487  6.26e- 1
##  3 G3           1.01e+ 0   0.0169     0.329  7.42e- 1
##  4 G4           1.20e+ 0   0.0147    12.7    1.11e-36
##  5 G5           9.69e- 1   0.0104    -3.02   2.55e- 3
##  6 G6           9.91e- 1   0.0134    -0.705  4.81e- 1
##  7 G7           1.02e+ 0   0.0132     1.77   7.64e- 2
##  8 G8           1.01e+ 0   0.00954    1.48   1.39e- 1
##  9 G9           9.59e- 1   0.0128    -3.31   9.20e- 4
## 10 G10          1.09e+ 0   0.0178     5.04   4.73e- 7
## 11 G11          1.08e+ 0   0.0130     6.04   1.52e- 9
## 12 Age_d[34,42) 9.93e- 1   0.186     -0.0358 9.71e- 1
## 13 Age_d[42,71] 9.76e- 1   0.185     -0.133  8.94e- 1
## 14 GenderMale   9.28e- 1   0.206     -0.363  7.16e- 1</code></pre>
<p>Let´s do a plot to figure out what is going on:</p>
<pre class="r"><code>tidy(eng_glm, exponentiate = T) %&gt;%
  ggplot(aes(x = term, y = estimate)) +
  geom_col(fill = &#39;steelblue&#39;) +
  coord_cartesian(ylim = c(0.8, 1.3)) +
  geom_text(aes(label = round(estimate, 2)), color = &#39;white&#39;, vjust = 1.5) +
  labs(title = &#39;Odds Ratio of Engagement&#39;) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p>The direct interpretation for Odds ratio is:</p>
<ol style="list-style-type: decimal">
<li>Male Leaders has 1.15 more engagement than Female Leads</li>
<li>Leaders between 34 and 42 years has 0.94 more engagement</li>
<li>Some who scores high on Dimention G4 has 1.17 more engagement</li>
</ol>
<p>Usually, odds ratio less than 1 are non statistically significant.</p>
<p>For Classification models, understanding <strong>accuracy</strong> is important.</p>
</div>
<div id="model-accuracy" class="section level2">
<h2>Model Accuracy</h2>
<p>A Classification report (includes: <strong>precision-specificity, recall-sensitivity and F1 score</strong>) is often used to measure the quality of predictions from a classification algorithm. How many predictions are true and how many are false. The classification report uses <strong>True Positive</strong>, <strong>True Negative</strong>, <strong>False Positive</strong> and <strong>False Negative</strong> in classification report generation.</p>
<ul>
<li>TP / True Positive: when an actual observation was positive and the model prediction is also positive</li>
<li>TN / True Negative: when an actual observation was negative and the model prediction is also negative</li>
<li>FP / False Positive: when an actual observation was negative but the model prediction is positive</li>
<li>FN / False Negative: when an actual observation was positive but the model prediction is negative</li>
</ul>
<p>We have already calculated the classification accuracy then the obvious question would be, what is the need for precision, recall and F1-score? The answer is <strong>accuracy is not a good measure when a class imbalance exists in the data set</strong>. A data set is said to be <strong>balanced</strong> if the dependent variable includes an approximately equal proportion of both classes (in binary classification case). For example, if the diabetes dataset includes 50% samples with diabetic and 50% non-diabetic patience, then the data set is said to be balanced and in such case, we can use accuracy as an evaluation metric. But in real-world it is often not the actual case.</p>
<p>Sometimes researchers also opt for <strong>AUC-ROC</strong> for model performance evaluation. AUC-ROC curve is a performance measurement for the classification problem at various thresholds settings. ROC tells how much model is capable of distinguishing between classes. Higher the AUC, better the model is at distinguishing between patients with diabetes and no diabetes.</p>
<p>An <strong>Confusion Matrix</strong> can be computed by using the <code>caret</code> package:</p>
<pre class="r"><code>message(&#39;Compute Confusion Matrix====================================&#39;)</code></pre>
<pre><code>## Compute Confusion Matrix====================================</code></pre>
<pre class="r"><code>#Predict Values:
y_hat = predict(eng_glm, eng_train, type = &#39;response&#39;)
y_hat = ifelse(y_hat &gt;= 0.5, 1, 0)
y_hat = factor(y_hat)

y_act = factor(eng_train$ENG)

#Compute Confusion Matrix
caret::confusionMatrix(y_hat, y_act, positive = &#39;1&#39;)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    0    1
##          0  545   99
##          1  156 1128
##                                           
##                Accuracy : 0.8677          
##                  95% CI : (0.8518, 0.8826)
##     No Information Rate : 0.6364          
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       
##                                           
##                   Kappa : 0.7091          
##                                           
##  Mcnemar&#39;s Test P-Value : 0.0004534       
##                                           
##             Sensitivity : 0.9193          
##             Specificity : 0.7775          
##          Pos Pred Value : 0.8785          
##          Neg Pred Value : 0.8463          
##              Prevalence : 0.6364          
##          Detection Rate : 0.5851          
##    Detection Prevalence : 0.6660          
##       Balanced Accuracy : 0.8484          
##                                           
##        &#39;Positive&#39; Class : 1               
## </code></pre>
<p>The model accuracy, the rate to correctly calssified posive and negative engagement, is around 85%. However, we have interest on the Leaders that have high engagement scores. Thus, we should use <strong>Sensitivity or Recall</strong>. This rate is ~91% meaning that the model has the hability to detect a Leader with positive engagemen 91% of the time.</p>
<p>Now, let´s compute confusion matrix on the test set:</p>
<pre class="r"><code>message(&#39;Compute Confusion Matrix====================================&#39;)</code></pre>
<pre><code>## Compute Confusion Matrix====================================</code></pre>
<pre class="r"><code>#Predict Values:
y_hat = predict(eng_glm, eng_test, type = &#39;response&#39;)
y_hat = ifelse(y_hat &gt;= 0.5, 1, 0)
y_hat = factor(y_hat)

y_act = factor(eng_test$ENG)

#Compute Confusion Matrix
caret::confusionMatrix(y_hat, y_act, positive = &#39;1&#39;)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction   0   1
##          0 184  44
##          1  56 359
##                                           
##                Accuracy : 0.8445          
##                  95% CI : (0.8141, 0.8716)
##     No Information Rate : 0.6267          
##     P-Value [Acc &gt; NIR] : &lt;2e-16          
##                                           
##                   Kappa : 0.6642          
##                                           
##  Mcnemar&#39;s Test P-Value : 0.2713          
##                                           
##             Sensitivity : 0.8908          
##             Specificity : 0.7667          
##          Pos Pred Value : 0.8651          
##          Neg Pred Value : 0.8070          
##              Prevalence : 0.6267          
##          Detection Rate : 0.5583          
##    Detection Prevalence : 0.6454          
##       Balanced Accuracy : 0.8287          
##                                           
##        &#39;Positive&#39; Class : 1               
## </code></pre>
</div>
<div id="a-more-complex-model" class="section level2">
<h2>A More Complex Model</h2>
<p>From the <strong>strategic</strong> point of view, focusing on Engagement Dimentions such as Recognition, Porpuse, Empowerment, Leadership is usefull. But, we usually do some actions based on the most important and relevant Question.</p>
<p>Now, let´s add questions to our analysis:</p>
<pre class="r"><code>message(&#39;Define the Model===============================================&#39;)</code></pre>
<pre><code>## Define the Model===============================================</code></pre>
<pre class="r"><code>model_Q = ENG ~ P5 + P6 + P7 + P8 + P9 + P10 + P11 + P12 + P13 + P14 + P15 + P16 + P17 + P18 + P19 + P20 + P21 + P22 + P23 + P24 + P25 + P26 + P27 + P28 + P29 + P30 + P31 + P32 + P33 + P34 + P35 + P36 + P37 + P38 + P39 + P40 + P41 + P42 + P43 + P44 + P45 + P46 + P47 + P48 + P49 + P50 + P51 + P52 + P53 + P54 + P55 + Age_d + Gender

#Estimate the Model
eng_glm_Q = glm(model_Q, data = eng_train, family = &#39;binomial&#39;)

#Plot the Odds Ratio
tidy(eng_glm_Q, exponentiate = T) %&gt;%
  ggplot(aes(x = term, y = estimate)) +
  geom_col(fill = &#39;steelblue&#39;) +
  coord_cartesian(ylim = c(0.8, 1.3)) +
  geom_text(aes(label = round(estimate, 2)), color = &#39;white&#39;, 
            vjust = 1.5, size = 2) +
  labs(title = &#39;Odds Ratio of Engagement&#39;) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))</code></pre>
<pre><code>## Warning: Removed 4 rows containing missing values (position_stack).</code></pre>
<pre><code>## Warning: Removed 4 rows containing missing values (geom_text).</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<p>What we have on this chart isn´t helpful. This is a very complex model with a lot of questions. More even, the maximun-likelihood algorithms did not succed to converge on values for some questions (i.e P32 and P50). This issue is related to the <strong>multicolinearity</strong> issue. Take a look on the correlation matrix we did and it is clear that several questions correlate with others. Making the maximun likelihood to fail.</p>
</div>
</div>
<div id="machine-learning-and-regularization" class="section level1">
<h1>Machine Learning and Regularization</h1>
<p>Regularization is a technique that helps with multicollinearity. In particular, we are going to use an algorithm call <strong>LASSO Regression.</strong> Before continue, you can check the <a href="https://eduardo0914rtutorials.netlify.app/post/2022/05/08/introduction-to-statistical-learning-regression-and-regularization/">Introduction to Statistical Learning Tutorial</a> in spanish or <a href="https://juliasilge.com/blog/lasso-the-office/">Julia´s Silge Blog</a>.</p>
<p>The objective function for logistic regression is the penalized negative binomial log-likelihood, and is</p>
<p><span class="math display">\[
\min_{(\beta_0, \beta) \in \mathbb{R}^{p+1}} -\left[\frac{1}{N} \sum_{i=1}^N y_i \cdot (\beta_0 + x_i^T \beta) - \log (1+e^{(\beta_0+x_i^T \beta)})\right] + \lambda \big[ (1-\alpha)\|\beta\|_2^2/2 + \alpha\|\beta\|_1\big].
\]</span></p>
<p>Logistic regression is often plagued with degeneracies when <span class="math inline">\(p &gt; N\)</span> and exhibits wild behavior even when <span class="math inline">\(N\)</span> is close to <span class="math inline">\(p\)</span>; the elastic net penalty alleviates these issues, and regularizes and selects variables as well. In this case, we will set up the elastic penalty <span class="math inline">\(\alpha = 1\)</span></p>
<p>First, we will split the data on train and test. For classification models the output variable must be a factor:</p>
<pre class="r"><code>#Convert ENG to a factor:
engagement_df$ENG = factor(engagement_df$ENG)

#Split data for training and test
eng_split = initial_split(engagement_df)
eng_train = training(eng_split)
eng_test = testing(eng_split)</code></pre>
<p>The Second step is to create a recipe and a model:</p>
<pre class="r"><code>message(&#39;Create a Model================================================&#39;)</code></pre>
<pre><code>## Create a Model================================================</code></pre>
<pre class="r"><code>set.seed(1234)



#Set up Engine
logistic_model &lt;- logistic_reg(penalty = tune(), mixture = 1) %&gt;% 
  set_engine(&quot;glmnet&quot;)

#setup Workflow
eng_wflow = workflow() %&gt;%
  add_model(logistic_model)


model_Q = ENG ~ P5 + P6 + P7 + P8 + P9 + P10 + P11 + P12 + P13 + P14 + P15 + P16 + P17 + P18 + P19 + P20 + P21 + P22 + P23 + P24 + P25 + P26 + P27 + P28 + P29 + P30 + P31 + P32 + P33 + P34 + P35 + P36 + P37 + P38 + P39 + P40 + P41 + P42 + P43 + P44 + P45 + P46 + P47 + P48 + P49 + P50 + P51 + P52 + P53 + P54 + P55 + Age_d + Gender

#Create folds
eng_cv = vfold_cv(eng_train, v = 10)


message(&#39;Create a Recipe================================================&#39;)</code></pre>
<pre><code>## Create a Recipe================================================</code></pre>
<pre class="r"><code>eng_recipe = recipe(model_Q,
                    data = eng_train) %&gt;%
                    step_center(all_numeric()) %&gt;%
                    step_scale(all_numeric()) %&gt;%
                    step_dummy(Gender, Age_d) %&gt;%
  # estimate the means and standard deviations
  prep(training = eng_train, retain = TRUE)


#setup Workflow
eng_wflow =eng_wflow %&gt;%
  add_recipe(eng_recipe)</code></pre>
<p>Now we are ready to fit the model:</p>
<pre class="r"><code>#Create a grid of potential values for lambda:
lambda_grid &lt;- grid_regular(penalty(), levels = 50)

message(&#39;Activate Parallel Processing====================================&#39;)</code></pre>
<pre><code>## Activate Parallel Processing====================================</code></pre>
<pre class="r"><code>doParallel::registerDoParallel()

set.seed(1234)
lasso_fit &lt;- tune_grid(eng_wflow,
                        resamples = eng_cv,
                        grid = lambda_grid,
                       control = control_grid(save_pred = TRUE),
                       metrics = metric_set(roc_auc))

message(&#39;Model Performance&#39;)</code></pre>
<pre><code>## Model Performance</code></pre>
<pre class="r"><code>autoplot(lasso_fit) +
  scale_x_log10(labels = scales::label_number()) +
  scale_y_log10(labels = scales::label_number())</code></pre>
<pre><code>## Scale for &#39;x&#39; is already present. Adding another scale for &#39;x&#39;, which will
## replace the existing scale.</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
<p>Now we can use <code>select_best</code> to choose the optimal value of <span class="math inline">\(\lambda\)</span>:</p>
<pre class="r"><code>max_accuracy = lasso_fit %&gt;%
  select_best(&#39;roc_auc&#39;)

eng_lasso_final = finalize_workflow(eng_wflow, max_accuracy)


message(&#39;Get LASSO Coefficients===========================================&#39;)</code></pre>
<pre><code>## Get LASSO Coefficients===========================================</code></pre>
<pre class="r"><code>lasso_params = eng_lasso_final %&gt;%
  fit(eng_train) %&gt;%
  extract_fit_parsnip()

tidy(lasso_params) %&gt;%
  filter(estimate != 0 &amp; term != &#39;(Intercept)&#39;) %&gt;%
  mutate(estimate = exp(estimate)) %&gt;%
  ggplot(aes(x = term, y = estimate)) +
  geom_col(fill = &#39;steelblue&#39;) +
  coord_cartesian(ylim = c(0.8, 1.8)) +
  geom_text(aes(label = round(estimate, 2)), color = &#39;white&#39;, 
            vjust = 1.5, size = 2) +
  labs(title = &#39;Odds Ratio of Engagement&#39;) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))</code></pre>
<pre><code>## Loaded glmnet 4.1-4</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<pre class="r"><code>message(&#39;plot variable importance=========================================&#39;)</code></pre>
<pre><code>## plot variable importance=========================================</code></pre>
<pre class="r"><code>library(vip)

eng_lasso_final %&gt;%
  fit(eng_train) %&gt;%
  extract_fit_parsnip() %&gt;%
  vip::vi(lambda = max_accuracy$penalty) %&gt;%
  mutate(Importance = abs(Importance),
         Variable = fct_reorder(Variable, Importance)) %&gt;%
  filter(Importance &gt; 0) %&gt;%
  ggplot(aes(x = Importance, y = Variable, fill = Sign)) +
  geom_col() +
  scale_x_continuous(expand = c(0, 0)) +
  labs(y = NULL)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-18-2.png" width="672" /></p>
<p>Now, instead of having 50 questions, we have around 19 questions to predict Engagement. Based on the <strong>Variable Importance Plot</strong> we figure of that P15, P48 and P117 are the top 3 Questions to focus.</p>
<p>Let´s compute accuracy and recall:</p>
<pre class="r"><code>#The last model
lambda = max_accuracy$penalty
eng_last_model = logistic_reg(penalty = 0.02329952, 
                              mixture = 1) %&gt;%
  set_engine(&#39;glmnet&#39;)

#The Last Workflow
eng_last_wf = eng_wflow %&gt;%
  update_model(eng_last_model)

#The Last Fit
eng_last_fit = eng_last_wf %&gt;%
  last_fit(eng_split)

fitdata = eng_last_fit[[5]][[1]]

caret::confusionMatrix(fitdata$.pred_class, fitdata$ENG, positive = &#39;1&#39;)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction   0   1
##          0 173  27
##          1  45 398
##                                           
##                Accuracy : 0.888           
##                  95% CI : (0.8611, 0.9113)
##     No Information Rate : 0.661           
##     P-Value [Acc &gt; NIR] : &lt; 2e-16         
##                                           
##                   Kappa : 0.745           
##                                           
##  Mcnemar&#39;s Test P-Value : 0.04513         
##                                           
##             Sensitivity : 0.9365          
##             Specificity : 0.7936          
##          Pos Pred Value : 0.8984          
##          Neg Pred Value : 0.8650          
##              Prevalence : 0.6610          
##          Detection Rate : 0.6190          
##    Detection Prevalence : 0.6890          
##       Balanced Accuracy : 0.8650          
##                                           
##        &#39;Positive&#39; Class : 1               
## </code></pre>
</div>
<div id="machine-learning-interpretability" class="section level1">
<h1>Machine Learning Interpretability</h1>
<p>There is no mathematical definition of interpretability. A (non-mathematical) definition I like by Miller (2017)¹⁰ is: Interpretability is the degree to which a human can understand the cause of a decision. Another one is: Interpretability is the degree to which a human can consistently predict the model’s result ¹¹. The higher the interpretability of a machine learning model, the easier it is for someone to comprehend why certain decisions or predictions have been made.</p>
<p>A model is better interpretable than another model if its decisions are easier for a human to comprehend than decisions from the other model. I will use both the terms interpretable and explainable interchangeably.</p>
<p>Methods for machine learning interpretability can be classified according to various criteria. Intrinsic or post hoc? This criteria distinguishes whether interpretability is achieved by restricting the complexity of the machine learning model (intrinsic) or by applying methods that analyze the model after training (post hoc).</p>
<p>Intrinsic interpretability refers to machine learning models that are considered interpretable due to their simple structure, such as short decision trees or sparse linear models.</p>
<p>Post hoc interpretability refers to the application of interpretation methods after model training. Permutation feature importance is, for example, a post hoc interpretation method. Post hoc methods can also be applied to intrinsically interpretable models. For example, permutation feature importance can be computed for decision trees.</p>
<p>Result of the interpretation method The various interpretation methods can be roughly differentiated according to their results.</p>
<ul>
<li><p>Feature summary statistic: Many interpretation methods provide summary statistics for each feature. Some methods return a single number per feature, such as feature importance, or a more complex result, such as the pairwise feature interaction strengths, which consist of a number for each feature pair.</p></li>
<li><p>Feature summary visualization: Most of the feature summary statistics can also be visualized. Some feature summaries are actually only meaningful if they are visualized and a table would be a wrong choice. The partial dependence of a feature is such a case. Partial dependence plots are curves that show a feature and the average predicted outcome. The best way to present partial dependences is to actually draw the curve instead of printing the coordinates.</p></li>
<li><p>Model internals (e.g. learned weights): The interpretation of intrinsically interpretable models falls into this category. Examples are the weights in linear models or the learned tree structure (the features and thresholds used for the splits) of decision trees. The lines are blurred between model internals and feature summary statistic in, for example, linear models, because the weights are both model internals and summary statistics for the features at the same time.</p></li>
<li><p>Data point: This category includes all methods that return data points (already existent or newly created) to make a model interpretable. One method is called counterfactual explanations. To explain the prediction of a data instance, the method finds a similar data point by changing some of the features for which the predicted outcome changes in a relevant way (e.g. a flip in the predicted class). Another example is the identification of prototypes of predicted classes. To be useful, interpretation methods that output new data points require that the data points themselves can be interpreted. This works well for images and texts, but is less useful for tabular data with hundreds of features.</p></li>
<li><p>Intrinsically interpretable model: One solution to interpreting black box models is to approximate them (either globally or locally) with an interpretable model. The interpretable model itself is interpreted by looking at internal model parameters or feature summary statistics.</p></li>
</ul>
<pre class="r"><code>require(DALEXtra)</code></pre>
<pre><code>## Loading required package: DALEXtra</code></pre>
<pre><code>## Loading required package: DALEX</code></pre>
<pre><code>## Welcome to DALEX (version: 2.4.1).
## Find examples and detailed introduction at: http://ema.drwhy.ai/</code></pre>
<pre><code>## 
## Attaching package: &#39;DALEX&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:ggmosaic&#39;:
## 
##     titanic</code></pre>
<pre><code>## The following object is masked from &#39;package:dplyr&#39;:
## 
##     explain</code></pre>
<pre class="r"><code>set.seed(1234)

#Set Variables on the Model
vip_features &lt;- c(&#39;P5&#39;, &#39;P6&#39;, &#39;P7&#39;, &#39;P8&#39;, &#39;P9&#39;, &#39;P10&#39;, &#39;P11&#39;, &#39;P12&#39;, &#39;P13&#39;, &#39;P14&#39;, &#39;P16&#39;, &#39;P18&#39;, &#39;P19&#39;, &#39;P20&#39;, &#39;P21&#39;, &#39;P22&#39;, &#39;P23&#39;, &#39;P24&#39;, &#39;P25&#39;, &#39;P26&#39;, &#39;P27&#39;, &#39;P28&#39;, &#39;P29&#39;, &#39;P30&#39;, &#39;P31&#39;, &#39;P32&#39;, &#39;P33&#39;, &#39;P34&#39;, &#39;P35&#39;, &#39;P36&#39;, &#39;P37&#39;, &#39;P38&#39;, &#39;P39&#39;, &#39;P40&#39;, &#39;P41&#39;, &#39;P42&#39;, &#39;P43&#39;, &#39;P44&#39;, &#39;P45&#39;, &#39;P46&#39;, &#39;P47&#39;, &#39;P49&#39;, &#39;P50&#39;, &#39;P51&#39;, &#39;P52&#39;, &#39;P53&#39;, &#39;P54&#39;, &#39;P55&#39;, &#39;Age_d&#39;, &#39;Gender&#39;,&#39;P15&#39;, &#39;P17&#39;, &#39;P48&#39;)

#Select variables
vip_train &lt;- 
  eng_train %&gt;% 
  select(all_of(vip_features))

#Fit the final model
fitm = eng_lasso_final %&gt;%
  fit(eng_train)


#Get the explainer
explainer_lm = explain_tidymodels(fitm, 
                                  data = vip_train, 
                                  y = eng_train$ENG,
                                  label = &quot;1&quot;,
                                  verbose = FALSE)</code></pre>
<pre><code>## Warning in Ops.factor(y, predict_function(model, data)): &#39;-&#39; not meaningful for
## factors</code></pre>
<pre class="r"><code>#Select Individual #200
manager &lt;- engagement_df[200, ]

lm_breakdown = predict_parts(explainer = explainer_lm, 
                             new_observation = manager)
#Plot Results
plot(lm_breakdown)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
</div>
